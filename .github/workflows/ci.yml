name: ci

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: "1 1 * * 1"

permissions:
  contents: write
  deployments: write

jobs:
  bench-go:
    runs-on: ubuntu-latest
    strategy:
        matrix:
          go-version: ["1.19"]

    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: ${{ matrix.go-version }}

      - uses: actions/cache@v3
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
            /tmp/geoJSON.zip
            ~/timezone.data
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Build
        run: make setup

      - name: Test
        run: |
          make test | grep "ops over" | tee benchmark_lotsa_result.txt

      - name: Bench
        run: |
          make bench | tee benchmark_result.txt

      - name: Continuous Benchmark
        uses: benchmark-action/github-action-benchmark@v1.17.0
        if: ${{ github.ref == 'refs/heads/main' }}
        with:
          name: Go Library Benchmark
          tool: "go"
          output-file-path: benchmark_result.txt
          github-token: ${{ secrets.CI_TOKEN }}
          gh-repository: "github.com/ringsaturn/tz-benchmark"
          auto-push: true
          alert-threshold: "150%"
          comment-on-alert: false
          fail-on-alert: false
          gh-pages-branch: "gh-pages"
          benchmark-data-dir-path: "docs/"
          alert-comment-cc-users: "@ringsaturn"

      - name: Setup benchmark file name
        id: gen-benchmark-file-name
        run: |
          echo "filename=tz_benchmark_go_${{ matrix.go-version }}.md" >> $GITHUB_OUTPUT

      - name: Report
        run: |
          set +e
          echo "## Go ${{ matrix.go-version }}" >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo "std benchmark:" >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo '```' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          cat benchmark_result.txt >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo '```' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          ehco '' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo "lotsa benchmark:" >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo '```' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          cat benchmark_lotsa_result.txt >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo '```' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}

      - name: Upload artifact
        uses: actions/upload-artifact@v3
        with:
          name: "benchmark_result_as_md"
          path: ${{ steps.gen-benchmark-file-name.outputs.filename }}

  build-python:
    runs-on: ubuntu-latest
    env:
      COLUMNS: 120
    strategy:
        matrix:
          python-version: ["3.9"]

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Build
        run: |
          pip install -r requirements.txt
          pip install pytest-memray

      - name: Setup benchmark file name
        id: gen-benchmark-file-name
        run: |
          echo "filename=tz_benchmark_py_${{ matrix.python-version }}.md" >> $GITHUB_OUTPUT

      - name: Bench Python
        run: |
          # ${{ steps.gen-benchmark-file-name.outputs.filename }}
          pytest tz_test.py --memray --benchmark-json output.json | tee benchmark_result.txt

      - name: Continuous Benchmark
        uses: benchmark-action/github-action-benchmark@v1.17.0
        if: ${{ github.ref == 'refs/heads/main' }}
        with:
          name: Python Library Benchmark
          tool: "pytest"
          output-file-path: output.json
          github-token: ${{ secrets.CI_TOKEN }}
          gh-repository: "github.com/ringsaturn/tz-benchmark"
          auto-push: true
          alert-threshold: "150%"
          comment-on-alert: false
          fail-on-alert: false
          gh-pages-branch: "gh-pages"
          benchmark-data-dir-path: "docs/"
          alert-comment-cc-users: "@ringsaturn"

      - name: Report
        run: |
          set +e
          echo "## Python ${{ matrix.python-version }}" >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo '```' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          cat benchmark_result.txt >> ${{ steps.gen-benchmark-file-name.outputs.filename }}
          echo '```' >> ${{ steps.gen-benchmark-file-name.outputs.filename }}

      - name: Upload artifact
        uses: actions/upload-artifact@v3
        with:
          name: "benchmark_result_as_md"
          path: ${{ steps.gen-benchmark-file-name.outputs.filename }}

  summary:
    needs: [bench-go, build-python]
    runs-on: ubuntu-latest
    steps:
      - name: Download a Build Artifact
        uses: actions/download-artifact@v3
        with:
          name: "benchmark_result_as_md"
          path: ""

      - name: Summary
        run: cat tz_benchmark_*.md > $GITHUB_STEP_SUMMARY
